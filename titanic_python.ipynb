{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Titanic_Python.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Titanic-Start",
        "colab_type": "text"
      },
      "source": [
        "###Titanic Python\n",
        "\n",
        "1/May/2019\n",
        "\n",
        "*\"first catch your hare\"*\n",
        "\n",
        "In our terms get the data and understand the objective. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Start-Code",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8b34a45e-9db6-4a6c-dfae-8a0d63a9567d"
      },
      "source": [
        "# simple statment to confirm that the notebook processing is active\n",
        "# for Google colab - it often takes a few seconds to get things started \n",
        "# this simple step with its clear output provides a visual check\n",
        "print (\"Titanic - Python\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Titanic - Python\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlvtwczK25f-",
        "colab_type": "text"
      },
      "source": [
        "### Imports\n",
        "\n",
        "One of the strengths of Python is the range of external libraries which can be used.\n",
        "\n",
        "An important mathematical library - which implements multidimensional arrays and many methods for manipulating them is numpy - which by convention we import as 'np'.  \n",
        "\n",
        "For data analysis there is an important library called pandas which implements a dataframe which can be used to store tabular data and implements many useful methods for manipulating these dataframes - by convention we import it as 'pd'.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GP-t4wSIVHYl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dzt8Zll-ZUnc",
        "colab_type": "text"
      },
      "source": [
        "Some of our libraries - particularly the presentation ones do require some initial set-up\n",
        "which is as well to do here with the import if it is 'standard' setup."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLe6rBloZVpE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mathplot sont size\n",
        "plt.rc(\"font\", size=14)\n",
        "\n",
        "# seaborn styles\n",
        "sns.set(style=\"white\") #white background style for seaborn plots\n",
        "sns.set(style=\"whitegrid\", color_codes=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YISzxq-xJz3F",
        "colab_type": "text"
      },
      "source": [
        "Import data from Google drive : \n",
        "\n",
        "For the next section of code to work you have to have copied the relevant files to your\n",
        "Google Drive.\n",
        "\n",
        "And as far as I know it only works from Google Colab - which is where I created this notebook.\n",
        "\n",
        "I have commented the code to prevent it running and generating an error if the Google Drive link is \n",
        "not in place -  it requires a Google Drive authorisation every session.\n",
        "\n",
        "Clearly if you uncomment and use this code then you need to skip or comment out the next section.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e4uiXXgVJV8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "\n",
        "# !ls /content/gdrive\n",
        "# !ls -l /content/gdrive/My\\ Drive\n",
        "# !ls -l /content/gdrive/My\\ Drive/Colab\\ Notebooks\n",
        "# !ls -l /content/gdrive/My\\ Drive/Colab\\ Notebooks/Titanic\n",
        "\n",
        "# titanic_known = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/Titanic/titanic_known.csv')\n",
        "# print (titanic_known.describe())\n",
        "# titanic_unknown = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/Titanic/titanic_unknown.csv')\n",
        "# print (titanic_unknown.describe())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R52r5Hy6K6nj",
        "colab_type": "text"
      },
      "source": [
        "Import data from a web source\n",
        "\n",
        "Pandas dataframes have a read_cvs method for reading csv sources - there are other simlar methods for reading different format sources all of which can take a web URL as the location as well as a local file.\n",
        "\n",
        "The below is a simple load directly from a web source - which is fine for small datasets but not recommended if the amount of data gets large.\n",
        "\n",
        "Any webserver will work.\n",
        "\n",
        "I use github to store my working code - you can access files in a github repository directly using the raw.githubusercontent.com/*username*/. . . address. \n",
        "\n",
        "Github also provides a 'special' repository name of *username*.github.io to allow you to create a repository which works as a static webserver (i.e. no server-side processing) - and this is perfectly serviceable for loading data into an object using the pandas read_csv method.\n",
        "\n",
        "After reading file into a pandas dateframe we use len to get the number of rows and the we use the datframe method count to get the number of entries in each collum - and the collum names - this is pretty much a  *'have I got the data I expect?'*  verification.  And it gives us some basic insight into our data.\n",
        "\n",
        "Whereas the whole purpose of using a machine to analyse data is to avoid much of the associated drudge work - it is hugely beneficial to have a good understanding of the data itself - particularly important is to make sure that the meaning of data is precise - for example, in this case we will have cause to ask what exactly does the Fare column contain.  And the asnwer to that question gives us an additional piece of data that we don't know that we have at the moment and that improves the performance of some of our models.\n",
        "\n",
        "To this end we will spend a bit of time just looking at the data in various ways."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhlDUtf0IPtt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "ce0b600e-fc0b-4f81-cc7b-1a91b4ae7527"
      },
      "source": [
        "titanic_known = pd.read_csv(\"https://raw.githubusercontent.com/johnsl01/Titanic_Python/master/titanic_known.csv\", sep=\",\")\n",
        "\n",
        "# titanic_known = pd.read_csv(\"https://johnsl01.github.io/titanic/titanic_known.csv\", sep=\",\")\n",
        "print (\"Known :\")\n",
        "print (str(len(titanic_known)) + \" Rows\")\n",
        "print (titanic_known.count())\n",
        "\n",
        "titanic_unknown = pd.read_csv(\"https://johnsl01.github.io/titanic/titanic_unknown.csv\", sep=\",\")\n",
        "print (\"\\nUnknown :\") \n",
        "print (str(len(titanic_unknown)) + \" Rows\")\n",
        "print (titanic_unknown.count())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Known :\n",
            "891 Rows\n",
            "PassengerId    891\n",
            "Survived       891\n",
            "Pclass         891\n",
            "Name           891\n",
            "Sex            891\n",
            "Age            714\n",
            "SibSp          891\n",
            "Parch          891\n",
            "Ticket         891\n",
            "Fare           891\n",
            "Cabin          204\n",
            "Embarked       889\n",
            "dtype: int64\n",
            "\n",
            "Unknown :\n",
            "418 Rows\n",
            "PassengerId    418\n",
            "Pclass         418\n",
            "Name           418\n",
            "Sex            418\n",
            "Age            332\n",
            "SibSp          418\n",
            "Parch          418\n",
            "Ticket         418\n",
            "Fare           417\n",
            "Cabin           91\n",
            "Embarked       418\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd09C_DDhQAe",
        "colab_type": "text"
      },
      "source": [
        "#### head and tail\n",
        "\n",
        "Our data is a bit unreal at the moment - we know the column names and the count of data in each - and we know that some ages and a lot of cabins are missing.\n",
        "\n",
        "And we know that the 'known' dataframe has an extra column; 'Survived' which we expect to tell us what the fate of that passenger was - which is obviously missing from the 'unknown' dataframe.\n",
        "\n",
        "One of the simplest things to do with data is to inspect it row by row and the starting point in this is usually to use the head and tail methods.  Head and tail derive from two unix commands which appeared very early in unix - and were used to show the first and last lines in a file.  By default they returned 5 lines and this default has persisted for almost 50 years as they are reincarnated in different systems.\n",
        "\n",
        "So pandas dataframes have a head and a tail method which do the same for dataframe rows : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3ggUQwWhQkm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "56cd6f40-89cd-4a05-d2da-b9fb2cc5b228"
      },
      "source": [
        "print (\"Known : -  first 5 and last 2:\")\n",
        "print (titanic_known.head())\n",
        "print (titanic_known.tail(2))\n",
        "\n",
        "print (\"UnkKnown : -  first 5 and last 2:\")\n",
        "print (titanic_unknown.head())\n",
        "print (titanic_unknown.tail(2))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Known : -  first 5 and last 2:\n",
            "   PassengerId  Survived  Pclass  ...     Fare Cabin  Embarked\n",
            "0            1         0       3  ...   7.2500   NaN         S\n",
            "1            2         1       1  ...  71.2833   C85         C\n",
            "2            3         1       3  ...   7.9250   NaN         S\n",
            "3            4         1       1  ...  53.1000  C123         S\n",
            "4            5         0       3  ...   8.0500   NaN         S\n",
            "\n",
            "[5 rows x 12 columns]\n",
            "     PassengerId  Survived  Pclass  ...   Fare Cabin  Embarked\n",
            "889          890         1       1  ...  30.00  C148         C\n",
            "890          891         0       3  ...   7.75   NaN         Q\n",
            "\n",
            "[2 rows x 12 columns]\n",
            "UnkKnown : -  first 5 and last 2:\n",
            "   PassengerId  Pclass  ... Cabin Embarked\n",
            "0          892       3  ...   NaN        Q\n",
            "1          893       3  ...   NaN        S\n",
            "2          894       2  ...   NaN        Q\n",
            "3          895       3  ...   NaN        S\n",
            "4          896       3  ...   NaN        S\n",
            "\n",
            "[5 rows x 11 columns]\n",
            "     PassengerId  Pclass                      Name  ...     Fare  Cabin  Embarked\n",
            "416         1308       3       Ware, Mr. Frederick  ...   8.0500    NaN         S\n",
            "417         1309       3  Peter, Master. Michael J  ...  22.3583    NaN         C\n",
            "\n",
            "[2 rows x 11 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T56ZUrexkHZs",
        "colab_type": "text"
      },
      "source": [
        "OK - so this tells us someting and also raises some concerns.\n",
        "\n",
        "The encoding of the survived column is 1 and 0  (1 for survived and 0 for died) - this is the most common numeric for a boolean value - 'Y' and 'N' and 'T' and 'F' being the most common character encodings - it is important to be sure that we get the mapping correct.  \n",
        "\n",
        "The Pclass column is the passenger class - the ship was divided into three different classes - 1st, 2nd and 3rd.  1st class passengers had luxurious cabins, they had open access to the upper decks and to the luxurious 'state' rooms of the ship - they were generally wealthier and as well as some having brought personal staff with them they had a lot of the ship's crew dedicated to their care.  We should expect that this will be a factor in survival - access and crew deference being significant factors. 2nd class had individual cabins but less deck access and restricted access to some areas.  3rd class passengers generally were below decks - had dormitory type accomodation and were packed in pretty tight - they did not have much deck access and were unable to get into much of the 1st and 2nd class areas.\n",
        "\n",
        "The name is the passenger's name and includes a title Mr. Mrs. etc as well as maiden names for married women.  This is text data so while we will want to make use of it we have to be careful in how we processes it - we will definitely want to pull out the title as that will help us in some cases with filling in the missing ages - and we will make some use of the name information to try and derive some information to help us with understanding what size of a party each passenger was likely in.\n",
        "\n",
        "Sex is text and is either 'male' or 'female'.  Any knowledge of the history of the titanic will likely include the phrase 'women and children first'.  We should expect that it is probably a factor in survival - and we can validate it pretty quickly.\n",
        "\n",
        "Age is self explanatory - it has a lot of missing values - we will look at mechanisms for filling in these values from the rest of the data, we expect that age will be a factor in survival - but need to explore how it affects it - expectation is probably that the elderly may have been less likely to survive - but despite being less able we expect the young may have survived both from having an adult caring for them and from the 'women and children first' policy - which is as yet unvalidated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxGMJiksbNo0",
        "colab_type": "text"
      },
      "source": [
        "Lets look at our spread of values in the numeric columns using the describe method.\n",
        "\n",
        "The PassengerID is just an incrementing counter which runs continuously from the knowns 1 through 891 and the unknowns (892 through 1309) - so the numerical analysis is pretty meaningless.\n",
        "\n",
        "The counts are the same as above, the missing values in the ages column are clear."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BU3gwsE2bOZh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "77d5b7cb-4256-4e87-e410-c7c56f7dca77"
      },
      "source": [
        "print (\"Known :\")\n",
        "print (titanic_known.describe())\n",
        "print (\"\\nUnknown :\") \n",
        "print (titanic_unknown.describe())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Known :\n",
            "       PassengerId    Survived      Pclass  ...       SibSp       Parch        Fare\n",
            "count   891.000000  891.000000  891.000000  ...  891.000000  891.000000  891.000000\n",
            "mean    446.000000    0.383838    2.308642  ...    0.523008    0.381594   32.204208\n",
            "std     257.353842    0.486592    0.836071  ...    1.102743    0.806057   49.693429\n",
            "min       1.000000    0.000000    1.000000  ...    0.000000    0.000000    0.000000\n",
            "25%     223.500000    0.000000    2.000000  ...    0.000000    0.000000    7.910400\n",
            "50%     446.000000    0.000000    3.000000  ...    0.000000    0.000000   14.454200\n",
            "75%     668.500000    1.000000    3.000000  ...    1.000000    0.000000   31.000000\n",
            "max     891.000000    1.000000    3.000000  ...    8.000000    6.000000  512.329200\n",
            "\n",
            "[8 rows x 7 columns]\n",
            "\n",
            "Unknown :\n",
            "       PassengerId      Pclass         Age       SibSp       Parch        Fare\n",
            "count   418.000000  418.000000  332.000000  418.000000  418.000000  417.000000\n",
            "mean   1100.500000    2.265550   30.272590    0.447368    0.392344   35.627188\n",
            "std     120.810458    0.841838   14.181209    0.896760    0.981429   55.907576\n",
            "min     892.000000    1.000000    0.170000    0.000000    0.000000    0.000000\n",
            "25%     996.250000    1.000000   21.000000    0.000000    0.000000    7.895800\n",
            "50%    1100.500000    3.000000   27.000000    0.000000    0.000000   14.454200\n",
            "75%    1204.750000    3.000000   39.000000    1.000000    0.000000   31.500000\n",
            "max    1309.000000    3.000000   76.000000    8.000000    9.000000  512.329200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQlKNxRVY6eG",
        "colab_type": "text"
      },
      "source": [
        "We have two groups of passengers - before we do anything with the data let's just check that the two sets are reasonably comparable.  By looking a a simple correlation matrix between the numeric collumns we should be able to see if the two groups are basically similar.\n",
        "\n",
        "We'll just run the correlation for the selected numeric columns - and we'll skip the 'Survived' column for the moment.  \n",
        "\n",
        "Clearly the Survived column's correlation with the other data is key to what we are trying to do - so we will focus on it when we have completed our exploration of the basic data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAGLolz5Yigg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "1adbb703-4b37-4868-b55e-3df37e190ead"
      },
      "source": [
        "print (\"Known :\")\n",
        "print (titanic_known[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']].corr())\n",
        "print (\"\\nUnknown :\") \n",
        "print (titanic_unknown[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']].corr())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Known :\n",
            "          Pclass       Age     SibSp     Parch      Fare\n",
            "Pclass  1.000000 -0.369226  0.083081  0.018443 -0.549500\n",
            "Age    -0.369226  1.000000 -0.308247 -0.189119  0.096067\n",
            "SibSp   0.083081 -0.308247  1.000000  0.414838  0.159651\n",
            "Parch   0.018443 -0.189119  0.414838  1.000000  0.216225\n",
            "Fare   -0.549500  0.096067  0.159651  0.216225  1.000000\n",
            "\n",
            "Unknown :\n",
            "          Pclass       Age     SibSp     Parch      Fare\n",
            "Pclass  1.000000 -0.492143  0.001087  0.018721 -0.577147\n",
            "Age    -0.492143  1.000000 -0.091587 -0.061249  0.337932\n",
            "SibSp   0.001087 -0.091587  1.000000  0.306895  0.171539\n",
            "Parch   0.018721 -0.061249  0.306895  1.000000  0.230046\n",
            "Fare   -0.577147  0.337932  0.171539  0.230046  1.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oVbrmDYaGOf",
        "colab_type": "text"
      },
      "source": [
        "This shows that our two groups are somewhat similar - but we have only looked at the numeric values.  This correlation ignores all the missing numeric values - and we know that a lot of ages are missing - a simple way to extend this analysis would be to encode gender and embarkation as integers and this would give us some additional information.\n",
        "\n",
        "However we will be doing this anyway at a later stage so we can look at the revised correlation then.  The purpose of this exercise is just familiarity with the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zn3ki8ahantT",
        "colab_type": "text"
      },
      "source": [
        "Let's go and have a look at these age values - there are a lot missing - let's look at how best to supply them.\n",
        "\n",
        "First how many are missing  in each of our datasets?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxTyIfJyamtq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "4413949e-d65c-455f-9c49-0374f235b39b"
      },
      "source": [
        "\n",
        "print(\"Known\")\n",
        "print(sum(pd.isnull(titanic_known['Age'])))\n",
        "print( str(round(sum(pd.isnull(titanic_known['Age']))/(len(titanic_known[\"PassengerId\"]))*100,2)) + \" %\")\n",
        "\n",
        "print(\"Unknown\")\n",
        "print(sum(pd.isnull(titanic_unknown['Age'])))\n",
        "print( str(round(sum(pd.isnull(titanic_unknown['Age']))/(len(titanic_unknown[\"PassengerId\"]))*100,2)) + \" %\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Known\n",
            "177\n",
            "19.87 %\n",
            "Unknown\n",
            "86\n",
            "20.57 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmyIowYpokZL",
        "colab_type": "text"
      },
      "source": [
        "OK - so pretty much the same percentage missing in each case - which is good as we want the knowns and unknowns to be similar othewise we have a bigger problem to solve.\n",
        "\n",
        "And let's look at the correlation between Age and the other numeric values to help understand the data:\n",
        "\n",
        "And we're just going to pullout the Age row from the correlation dataframe - so it comes out as a series:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4obik4EolFb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "3c99e365-1efa-419c-da39-328c188d6e62"
      },
      "source": [
        "print (\"Known :\")\n",
        "print (titanic_known[['Age','Pclass', 'SibSp', 'Parch', 'Fare','Survived']].corr().loc['Age',:])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Known :\n",
            "Age         1.000000\n",
            "Pclass     -0.369226\n",
            "SibSp      -0.308247\n",
            "Parch      -0.189119\n",
            "Fare        0.096067\n",
            "Survived   -0.077221\n",
            "Name: Age, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3_Q6rBBb_sv",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "We have a choice of approaches here - we can fill in the knowns with a variety of methods and see which one gives us the better result - but the differences between the aproaches to filling the data will probably be hidden by other effects. Alternatively we can treat both datasets together and attempt to fill in the values in both at the same time - which precludes using survival as a factor in filling in age - which is a somewhat circular approach anyway.\n",
        "\n",
        "In terms of approches to the actual value there are single value approaches - populate all the missing values with a single value - and typically these will use either the mean (for regular distributed data) or the median (for skewed data) - or some other justifiable value based on the data.  And there are multiple value approaches which in turn divide down into subset versions of the single value approach - for example in this case take the mean of ages for each class and apply it to the missing values for that class (note the correlation between age and class above as a justification for this approach) - similarly for gender, for example, or get sophisticated and divide into gender and class and fill each with the relevant mean, which we can't currenlty justify until we know whether there is a supporting correlation.  \n",
        "\n",
        "And finally there are existing algorithms for filling in missing data using the other data for each passenger which is really just a sophisticated version of the divide and fill just mentioned.  Of these MICE (Multiple (or Multivariate) Imputation by Chained Equations) is the most popular sophisticated date filling algorithm in use at the moment and there are pythons libraries to support it.\n",
        "\n",
        "For the moment we will use a dumb approach across the entire dataset - and simply apply a rounded mean age.\n",
        "\n",
        "The reason to not use MICE at this point is that we will tease out some more data - specifically we can pull out the titles and use these as additional input - for male passengers we know that 'Master' vs 'Mr.' will correlate with age, without even having to look at the data and to a lesser extent 'Miss' vs 'Mrs' will do so for females - the underlying argument here is that to make good use of a sophisticated tool you should invest in good data prep and make it pay. \n",
        "\n",
        "So single value for knowns and unknowns or one for each - much of this exercise is based on an assumption that the unknowns are a good selection from the data, which we have partially validated with the correlation exercise above so the mean of all the supplied ages used to fill all the missing ones is the initial cheap and easy approach.\n",
        "\n",
        "And we'll come back around to this later to do a better job.\n",
        "\n",
        "Just one additional point to consder here - our currrent problem is to find ways to populate the survived column in the unknowns based on the available data and using the known survived to help us build and test the aproaches.  We could turn this problem around and try to find the ages for all passengers in a data set where there is a boollean collum of survived which happens to have a bunch of missing values.  So data wrangling to prepare your data is in some case the same class of a problem as the actual analytics exercise itself.  And there is very little point in throwing a sophisticated analytics method at data that you have already polluted by using low tech preparation tools on."
      ]
    }
  ]
}